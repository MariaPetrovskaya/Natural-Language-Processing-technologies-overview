# Word2Vec

##Теоретические заметки

Про модель  word2vec. Она была предложена Томашем Миколавом в 2013 году[1] и привела к настоящему взрыву интереса к дистрибутивной семантике. В основе подхода — моделирование условного распределения вероятностей соседних слов. Важная особенность и отличие от предыдущей модели — в том что, word2vec работает с локальным контекстом, то есть с окном небольшой длины. Например если ширина равна трём, то мы будем идти по тексту и поочереди выбирать вот такие окна, то есть мы поочереди будем каждое слово ставить в центр окна, и рассматривать его контекст. А также, на каждом шаге, для каждого окна, мы обновляем параметры модели, чтобы повысить правдоподобие того, что мы сейчас наблюдаем. Томаш предложил два варианта модели. Первый называется Skip Gram — он моделирует распределение соседей при условии центрального слова. Второй вариант (CBOW)— наоборот, моделирует распределение центрального слова при условии известных соседей. В модели для каждого слова хранятся и настраиваются два вектора. Первый (мы будем называть его центральным) мы будем использовать, когда слово находится в центре окна. Второй — когда слово является контекстом, то есть — не в центре. Параметры этой модели настраиваются градиентным спуском. По сути, процесс обучения word2vec идентичен обучению обычной нейросети, когда подаются обучающие примеры (в данном случае окна) один за другим, и после наблюдений небольшой пачки примеров веса модели обновляются.

[1] Mikolov T. et al. Efficient estimation of word representations in vector space //arXiv preprint arXiv:1301.3781. – 2013. (https://arxiv.org/abs/1301.3781)

Давайте немного подробнее остановимся на варианте Skip Gram. Мы моделируем соседние слова в окне при условии известного центрального слова. В отличие от предыдущей модели, вот эта суммы идёт не по всем уникальным словам, а по всем возможным окнам в корпусе, то есть по всем словоупотреблениям — каждое словоупотребление становится центром окна, мы берём контекст в этом окне, оцениваем его правдоподобие и обновляем веса. Чтобы было удобнее работать с таким распределением, мы предполагаем, что соседние слова условно независимы друг от друга, когда мы уже пронаблюдали центральное слово. Тогда наше распределение факторизуется, то есть его можно представить в виде произведения более простых распределений. Дальше мы моделируем такие распределения по-отдельности, независимо друг от друга. Моделируем мы их с помощью старого доброго друга — софтмакса. Внутри софтмакса мы используем скалярное произведение вектора текущего центрального слова с векторами остальных слов. Всё вроде бы хорошо, но в знаменателе у нас стоит сумма по всему словарю, а градиентные шаги нам нужно делать для каждого окна. Это очень дорого с вычислительной точки зрения. Томаш не растерялся и предложил аппроксимировать честный софтмакс более дешёвыми вариантами — один такой способ называется "отрицательным сэмплированием" (или "negative sampling"). Идея в том, что сумму в знаменателе мы считаем не по всему словарю, а по небольшому числу случайно выбранных слов. Эти слова мы выбираем каждый раз заново. Проблема снижения вычислительной сложности софтмакса является очень насущной, поэтому были предложены и другие варианты — например, иерархический софтмакс[1]. Но, пока что, мы не будем их рассматривать.

[1] Hierarchical softmax and negative sampling: short notes worth telling

https://arxiv.org/ftp/arxiv/papers/1206/1206.6426.pdf

Спустя несколько лет модель word2vec получила вполне естественное развитие. Важной особенностью естественных языков является то, что слова могут принимать различные формы, при этом не меняя смысла. Ну, а мы же хотим чтобы наши вектора описывали именно смысл. Другими, словами мы хотим получить инвариантность к словоизменению. Один способ получить такую инвариантность — это нормализовать текст перед обучением модели — например, с помощью стэмминга или лемматизации. Однако первое ненадёжно, а второе сложно — не для всех языков есть хорошие морфологические анализаторы и лемматизаторы. На сайте "rusvectores.org" выложено много обученных моделей для русского языка, а также на этом сайте можно поисследовать возможности этих моделей в интерактивном режиме. Например, поискать слова, наиболее похожие на слово-запрос. Cреди похожих слов для запроса "язык", нет ничего связанного с биологией, только лингвистические термины. Это связано с тем, что word2vec для каждого слова хранит только один вектор, и это может привести к тому, что некоторые смыслы слова потеряются, а вектор будет описывать только наиболее частотный смысл. А ещё можно выбрать несколько слов и посмотреть графически, как они соотносятся друг с другом. Например, видно, что слово "молоток" ближе к глаголу "забивать", чем к глаголу "любить", например. А глагол "пить" находится недалеко от объектов действия "чай", "кофе". Другое свойство заключается в том, что мы можем складывать и вычитать вектора слов, при этом, как бы, переходя по семантическим связям. Например, если мы вычтем из вектора для снова "женщина" вектор для слова "мужчина", а потом прибавим вектор для слова "дядя" и попробуем поискать ближайшие вектора к полученному, то найдем слова "тётя". Это работает не для всех отношений не для всех слов, но для некоторых частотных — работает.
